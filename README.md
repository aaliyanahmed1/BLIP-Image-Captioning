# BLIP-Image-Captioning

# ğŸ§ âœ¨ BLIP Transformer-Based Image Captioning ğŸ”ğŸ–¼ï¸

A powerful and elegant image captioning system built with the cutting-edge **BLIP (Bootstrapping Language-Image Pretraining)** and **Vision Transformers (ViT)** â€” enabling machines to "see" and **describe images just like a human** would!

![BLIP Model](https://raw.githubusercontent.com/salesforce/BLIP/main/demo/blip_logo.png)
<sub>Image credit: Salesforce Research</sub>

---

## ğŸš€ Features

âœ… Generates **natural language captions** for any image  
âœ… Leverages **Vision Transformers** for accurate scene understanding  
âœ… Built with **Hugging Face Transformers**, **PyTorch**, and **BLIP**  
âœ… Supports real-time or batch image captioning  
âœ… Easily extensible for **Visual Question Answering** or **multimodal reasoning**

---

## ğŸ–¼ï¸ Demo
![Demo Input](https://github.com/aaliyanahmed1/BLIP-Image-Captioning/blob/main/input.jpg?raw=true)



## ğŸ“¦ Installation

```bash
git clone https://github.com/yourusername/blip-image-captioning.git
cd blip-image-captioning
pip install -r requirements.txt
